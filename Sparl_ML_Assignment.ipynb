{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3010ebf4",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "\n",
    "\n",
    "<center><h1> Spark ML Assingment </h1></center>\n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f66ef2a",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "#### `IMPORTING THE REQUIRED LIBRARIES`\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8c60c133",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip3 install matplotlib --user\n",
    "# !pip3 install seaborn --user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ab39e716",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing the required libraries\n",
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.types as tp\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "# # libraries to make plots\n",
    "# import matplotlib.pyplot as plt\n",
    "# %matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7d302d45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "# import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1f7bad3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create spark session\n",
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4d7092f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reading the train data\n",
    "train_data = spark.read.csv(\"ml_assignment/train.csv\",inferSchema=True, header=True)\n",
    "\n",
    "# reading the test data\n",
    "test_data  = spark.read.csv(\"ml_assignment/test.csv\", inferSchema=True, header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d13c12f9",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# `DATA EXPLORATION`\n",
    "\n",
    "Now, that we have read the data, the first step is to explore the data. We will try to find out the following things from the data.\n",
    "\n",
    "---\n",
    "**`1. Variable Identification`**\n",
    "\n",
    "- Data Type of the columns\n",
    "\n",
    "**`2. Univariate Analysis`**\n",
    "- 1. Average Purchase amount?\n",
    "- 2. Counting and Removing null values\n",
    "- 3. How many distinct values per column?\n",
    "- 4. Count category values within each of the following column:\n",
    "     - Gender\n",
    "     - Age\n",
    "     - City_Category\n",
    "     - Stay_In_Current_City_Years\n",
    "     - Marital_Status\n",
    "\n",
    "**`3. Bivariate Analysis`**\n",
    "\n",
    "- Calculate average Purchase for each of the following columns:\n",
    "     - Gender\n",
    "     - Age\n",
    "     - City_Category\n",
    "     - Stay_In_Current_City_Years\n",
    "     - Marital_Status\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69c0aeca",
   "metadata": {},
   "source": [
    "##  `VARIABLE INDENTIFICATION`\n",
    "\n",
    "#### `Data Type of the Columns`\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4ddc8ee7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- User_ID: integer (nullable = true)\n",
      " |-- Product_ID: string (nullable = true)\n",
      " |-- Gender: string (nullable = true)\n",
      " |-- Age: string (nullable = true)\n",
      " |-- Occupation: integer (nullable = true)\n",
      " |-- City_Category: string (nullable = true)\n",
      " |-- Stay_In_Current_City_Years: string (nullable = true)\n",
      " |-- Marital_Status: integer (nullable = true)\n",
      " |-- Product_Category_1: integer (nullable = true)\n",
      " |-- Product_Category_2: integer (nullable = true)\n",
      " |-- Product_Category_3: integer (nullable = true)\n",
      " |-- Purchase: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# data type of the columns\n",
    "train_data.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e6428b69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+------+-----+----------+-------------+--------------------------+--------------+------------------+------------------+------------------+--------+\n",
      "|User_ID|Product_ID|Gender|  Age|Occupation|City_Category|Stay_In_Current_City_Years|Marital_Status|Product_Category_1|Product_Category_2|Product_Category_3|Purchase|\n",
      "+-------+----------+------+-----+----------+-------------+--------------------------+--------------+------------------+------------------+------------------+--------+\n",
      "|1000001| P00069042|     F| 0-17|        10|            A|                         2|             0|                 3|              null|              null|    8370|\n",
      "|1000001| P00248942|     F| 0-17|        10|            A|                         2|             0|                 1|                 6|                14|   15200|\n",
      "|1000001| P00087842|     F| 0-17|        10|            A|                         2|             0|                12|              null|              null|    1422|\n",
      "|1000001| P00085442|     F| 0-17|        10|            A|                         2|             0|                12|                14|              null|    1057|\n",
      "|1000002| P00285442|     M|  55+|        16|            C|                        4+|             0|                 8|              null|              null|    7969|\n",
      "|1000003| P00193542|     M|26-35|        15|            A|                         3|             0|                 1|                 2|              null|   15227|\n",
      "|1000004| P00184942|     M|46-50|         7|            B|                         2|             1|                 1|                 8|                17|   19215|\n",
      "|1000004| P00346142|     M|46-50|         7|            B|                         2|             1|                 1|                15|              null|   15854|\n",
      "|1000004|  P0097242|     M|46-50|         7|            B|                         2|             1|                 1|                16|              null|   15686|\n",
      "|1000005| P00274942|     M|26-35|        20|            A|                         1|             1|                 8|              null|              null|    7871|\n",
      "|1000005| P00251242|     M|26-35|        20|            A|                         1|             1|                 5|                11|              null|    5254|\n",
      "|1000005| P00014542|     M|26-35|        20|            A|                         1|             1|                 8|              null|              null|    3957|\n",
      "|1000005| P00031342|     M|26-35|        20|            A|                         1|             1|                 8|              null|              null|    6073|\n",
      "|1000005| P00145042|     M|26-35|        20|            A|                         1|             1|                 1|                 2|                 5|   15665|\n",
      "|1000006| P00231342|     F|51-55|         9|            A|                         1|             0|                 5|                 8|                14|    5378|\n",
      "|1000006| P00190242|     F|51-55|         9|            A|                         1|             0|                 4|                 5|              null|    2079|\n",
      "|1000006|  P0096642|     F|51-55|         9|            A|                         1|             0|                 2|                 3|                 4|   13055|\n",
      "|1000006| P00058442|     F|51-55|         9|            A|                         1|             0|                 5|                14|              null|    8851|\n",
      "|1000007| P00036842|     M|36-45|         1|            B|                         1|             1|                 1|                14|                16|   11788|\n",
      "|1000008| P00249542|     M|26-35|        12|            C|                        4+|             1|                 1|                 5|                15|   19614|\n",
      "+-------+----------+------+-----+----------+-------------+--------------------------+--------------+------------------+------------------+------------------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_data.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42c3c2c7",
   "metadata": {},
   "source": [
    "## `UNIVARIATE ANALYSIS`\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e85ead86",
   "metadata": {},
   "source": [
    "#### `Exploring the Target Variable`\n",
    "\n",
    "---\n",
    "\n",
    "The `Target Variable` for our use-case is `Purchase` and this column has `numeric` data type.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "925cf54e",
   "metadata": {},
   "source": [
    "---\n",
    "`1. Average Purchase Amount`\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "36751f97",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>Avg_Purchase</th></tr>\n",
       "<tr><td>9263.968712959126</td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "+-----------------+\n",
       "|     Avg_Purchase|\n",
       "+-----------------+\n",
       "|9263.968712959126|\n",
       "+-----------------+"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.agg(F.avg(\"Purchase\").alias('Avg_Purchase'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c079b4fc",
   "metadata": {},
   "source": [
    "---\n",
    "` 2. Counting and Removing null values `\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "79fefe26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User_ID 0\n",
      "Product_ID 0\n",
      "Gender 0\n",
      "Age 0\n",
      "Occupation 0\n",
      "City_Category 0\n",
      "Stay_In_Current_City_Years 0\n",
      "Marital_Status 0\n",
      "Product_Category_1 0\n",
      "Product_Category_2 173638\n",
      "Product_Category_3 383247\n",
      "Purchase 0\n"
     ]
    }
   ],
   "source": [
    "### null values in each column\n",
    "for c in train_data.columns:\n",
    "    # define the condition\n",
    "    missing_values = F.isnull(c)\n",
    "    \n",
    "    # filter the data with condition and count the number of data points\n",
    "    missing_values = train_data.filter(missing_values).count()\n",
    "    \n",
    "    # print the result\n",
    "    print(c, missing_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4f702ca",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "So, we have not much null values in most of the columns:\n",
    "\n",
    " * **`User_ID`**:   0\n",
    " * **`Product_ID`**:   0\n",
    " * **`Gender`**:   0\n",
    " * **`Age`**:  0\n",
    " * **`Occupation`**:  0\n",
    " * **`City_Category`**:  0\n",
    " * **`Stay_In_Current_City_Years`**:  0\n",
    " * **`Marital_Status`**:  0\n",
    " * **`Product_Category_1`**:  0\n",
    " * **`Product_Category_2`**: 173638\n",
    " * **`Product_Category_3`**:  383247\n",
    " * **`Purchase`**:  0\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f3004f62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+-------------------------+\n",
      "|Product_Category_2|count(Product_Category_2)|\n",
      "+------------------+-------------------------+\n",
      "|              null|                        0|\n",
      "|                 2|                    49217|\n",
      "|                 3|                     2884|\n",
      "|                 4|                    25677|\n",
      "|                 5|                    26235|\n",
      "|                 6|                    16466|\n",
      "|                 7|                      626|\n",
      "|                 8|                    64088|\n",
      "|                 9|                     5693|\n",
      "|                10|                     3043|\n",
      "|                11|                    14134|\n",
      "|                12|                     5528|\n",
      "|                13|                    10531|\n",
      "|                14|                    55108|\n",
      "|                15|                    37855|\n",
      "|                16|                    43255|\n",
      "|                17|                    13320|\n",
      "|                18|                     2770|\n",
      "+------------------+-------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "product_category_1=train_data.groupBy(\"Product_Category_2\").agg(F.count(\"Product_Category_2\"))\n",
    "product_category_1.orderBy('Product_Category_2',ascending=True).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f82ac00",
   "metadata": {},
   "source": [
    "#### The most common values in Product_Category_2 is 8. So,we can fill the null values with null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f5fce56f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+-------------------------+\n",
      "|Product_Category_3|count(Product_Category_3)|\n",
      "+------------------+-------------------------+\n",
      "|              null|                        0|\n",
      "|                 3|                      613|\n",
      "|                 4|                     1875|\n",
      "|                 5|                    16658|\n",
      "|                 6|                     4890|\n",
      "|                 8|                    12562|\n",
      "|                 9|                    11579|\n",
      "|                10|                     1726|\n",
      "|                11|                     1805|\n",
      "|                12|                     9246|\n",
      "|                13|                     5459|\n",
      "|                14|                    18428|\n",
      "|                15|                    28013|\n",
      "|                16|                    32636|\n",
      "|                17|                    16702|\n",
      "|                18|                     4629|\n",
      "+------------------+-------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "product_category_3=train_data.groupBy(\"Product_Category_3\").agg(F.count(\"Product_Category_3\"))\n",
    "product_category_3.orderBy('Product_Category_3',ascending=True).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb44c9dd",
   "metadata": {},
   "source": [
    "#### The most common values in Product_Category_3 is __16__. So, we can fill the null values with __16__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a3d94823",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fill null values in the train data\n",
    "train_data = train_data.fillna({\n",
    "            \"Product_Category_2\" : 8,\n",
    "            \"Product_Category_3\": 16,\n",
    "        })"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17b0acb1",
   "metadata": {},
   "source": [
    "### Check Again, and see the null values are filled or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fd567d06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User_ID 0\n",
      "Product_ID 0\n",
      "Gender 0\n",
      "Age 0\n",
      "Occupation 0\n",
      "City_Category 0\n",
      "Stay_In_Current_City_Years 0\n",
      "Marital_Status 0\n",
      "Product_Category_1 0\n",
      "Product_Category_2 0\n",
      "Product_Category_3 0\n",
      "Purchase 0\n"
     ]
    }
   ],
   "source": [
    "### null values in each column\n",
    "for c in train_data.columns:\n",
    "    # define the condition\n",
    "    missing_values = F.isnull(c)\n",
    "    \n",
    "    # filter the data with condition and count the number of data points\n",
    "    missing_values = train_data.filter(missing_values).count()\n",
    "    \n",
    "    # print the result\n",
    "    print(c, missing_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78ed25a1",
   "metadata": {},
   "source": [
    "- Here, we can see that there are no null values are present in the dataset.\n",
    "- So, we can also fill the null values in test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6e99702d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User_ID 0\n",
      "Product_ID 0\n",
      "Gender 0\n",
      "Age 0\n",
      "Occupation 0\n",
      "City_Category 0\n",
      "Stay_In_Current_City_Years 0\n",
      "Marital_Status 0\n",
      "Product_Category_1 0\n",
      "Product_Category_2 72344\n",
      "Product_Category_3 162562\n"
     ]
    }
   ],
   "source": [
    "### null values in each column of test dataset.\n",
    "for c in test_data.columns:\n",
    "    # define the condition\n",
    "    missing_values = F.isnull(c)\n",
    "    \n",
    "    # filter the data with condition and count the number of data points\n",
    "    missing_values = test_data.filter(missing_values).count()\n",
    "    \n",
    "    # print the result\n",
    "    print(c, missing_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "becc6d12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fill null values in the train data\n",
    "test_data = test_data.fillna({\n",
    "            \"Product_Category_2\" : 8,\n",
    "            \"Product_Category_3\": 16,\n",
    "        })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "406802bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User_ID 0\n",
      "Product_ID 0\n",
      "Gender 0\n",
      "Age 0\n",
      "Occupation 0\n",
      "City_Category 0\n",
      "Stay_In_Current_City_Years 0\n",
      "Marital_Status 0\n",
      "Product_Category_1 0\n",
      "Product_Category_2 0\n",
      "Product_Category_3 0\n"
     ]
    }
   ],
   "source": [
    "### null values in each column of test dataset.\n",
    "for c in test_data.columns:\n",
    "    # define the condition\n",
    "    missing_values = F.isnull(c)\n",
    "    \n",
    "    # filter the data with condition and count the number of data points\n",
    "    missing_values = test_data.filter(missing_values).count()\n",
    "    \n",
    "    # print the result\n",
    "    print(c, missing_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08758b5e",
   "metadata": {},
   "source": [
    "---\n",
    "` 3. How many distinct values per column? `\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "436f8981",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>User_ID</th><th>Product_ID</th><th>Gender</th><th>Age</th><th>Occupation</th><th>City_Category</th><th>Stay_In_Current_City_Years</th><th>Marital_Status</th><th>Product_Category_1</th><th>Product_Category_2</th><th>Product_Category_3</th><th>Purchase</th></tr>\n",
       "<tr><td>5891</td><td>3631</td><td>2</td><td>7</td><td>21</td><td>3</td><td>5</td><td>2</td><td>20</td><td>17</td><td>15</td><td>18105</td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "+-------+----------+------+---+----------+-------------+--------------------------+--------------+------------------+------------------+------------------+--------+\n",
       "|User_ID|Product_ID|Gender|Age|Occupation|City_Category|Stay_In_Current_City_Years|Marital_Status|Product_Category_1|Product_Category_2|Product_Category_3|Purchase|\n",
       "+-------+----------+------+---+----------+-------------+--------------------------+--------------+------------------+------------------+------------------+--------+\n",
       "|   5891|      3631|     2|  7|        21|            3|                         5|             2|                20|                17|                15|   18105|\n",
       "+-------+----------+------+---+----------+-------------+--------------------------+--------------+------------------+------------------+------------------+--------+"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# distinct values in each column\n",
    "train_data.agg(*(F.countDistinct(F.col(c)).alias(c) for c in train_data.columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfca0ca3",
   "metadata": {},
   "source": [
    "---\n",
    "` 4. Count category values within each of the following column: `\n",
    "--- \n",
    "- **Gender**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d9166d56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+----------+\n",
      "|Gender| Count|Percentage|\n",
      "+------+------+----------+\n",
      "|     F|135809|   24.6895|\n",
      "|     M|414259|   75.3105|\n",
      "+------+------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# number of data points in each category and their percentage\n",
    "train_data.groupBy(\"Gender\").agg(F.count(\"Gender\").alias(\"Count\"),\n",
    "                                           F.round((F.count(\"Gender\")/train_data.count())*100, 4)\n",
    "                                 .alias(\"Percentage\")).show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8fa3f99",
   "metadata": {},
   "source": [
    "- Here, we can see that Gender __Male__ is Higher than __Female.__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f228841",
   "metadata": {},
   "source": [
    "- **Age**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "22ac5a07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+----------+\n",
      "|  Age| Count|Percentage|\n",
      "+-----+------+----------+\n",
      "|18-25| 99660|   18.1178|\n",
      "|26-35|219587|     39.92|\n",
      "| 0-17| 15102|    2.7455|\n",
      "|46-50| 45701|    8.3082|\n",
      "|51-55| 38501|    6.9993|\n",
      "|36-45|110013|   19.9999|\n",
      "|  55+| 21504|    3.9093|\n",
      "+-----+------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# number of data points in each category and their percentage\n",
    "train_data.groupBy(\"Age\").agg(F.count(\"Age\").alias(\"Count\"),\n",
    "                                           F.round((F.count(\"Age\")/train_data.count())*100, 4)\n",
    "                                 .alias(\"Percentage\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbbe447a",
   "metadata": {},
   "source": [
    "- Here, we can see that people age between 25 to 35 are higher number than other.\n",
    "- And, Next one is age between 36 to 45 is second higher."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e79830aa",
   "metadata": {},
   "source": [
    "- **City_Category**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1c3a5de4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+------+----------+\n",
      "|City_Category| Count|Percentage|\n",
      "+-------------+------+----------+\n",
      "|            B|231173|   42.0263|\n",
      "|            C|171175|   31.1189|\n",
      "|            A|147720|   26.8549|\n",
      "+-------------+------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# number of data points in each category and their percentage\n",
    "train_data.groupBy(\"City_Category\").agg(F.count(\"City_Category\").alias(\"Count\"),\n",
    "                                           F.round((F.count(\"City_Category\")/train_data.count())*100, 4)\n",
    "                                 .alias(\"Percentage\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f39b2ee",
   "metadata": {},
   "source": [
    "- Here, we can see that the City_Category B is higher number than others.\n",
    "- And, next one is City_Category C which is second higher."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48de7c9f",
   "metadata": {},
   "source": [
    "- **Stay_In_Current_City_Years**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c6332d7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------+------+----------+\n",
      "|Stay_In_Current_City_Years| Count|Percentage|\n",
      "+--------------------------+------+----------+\n",
      "|                         3| 95285|   17.3224|\n",
      "|                         0| 74398|   13.5252|\n",
      "|                        4+| 84726|   15.4028|\n",
      "|                         1|193821|   35.2358|\n",
      "|                         2|101838|   18.5137|\n",
      "+--------------------------+------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# number of data points in each category and their percentage\n",
    "train_data.groupBy(\"Stay_In_Current_City_Years\").agg(F.count(\"Stay_In_Current_City_Years\").alias(\"Count\"),\n",
    "                                           F.round((F.count(\"Stay_In_Current_City_Years\")/train_data.count())*100, 4)\n",
    "                                 .alias(\"Percentage\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b49e990",
   "metadata": {},
   "source": [
    "- Here, we can see that the most of the people stay in current city is __2__\n",
    "- And, Next one the second most of the people stay in current city is __3__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef8a59f1",
   "metadata": {},
   "source": [
    "- **Marital_Status**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c0c0781e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+------+----------+\n",
      "|Marital_Status| Count|Percentage|\n",
      "+--------------+------+----------+\n",
      "|             1|225337|   40.9653|\n",
      "|             0|324731|   59.0347|\n",
      "+--------------+------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_data.groupBy(\"Marital_Status\").agg(F.count(\"Marital_Status\").alias(\"Count\"),\n",
    "                                           F.round((F.count(\"Marital_Status\")/train_data.count())*100, 4)\n",
    "                                 .alias(\"Percentage\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8650ae63",
   "metadata": {},
   "source": [
    "- Here, we can see that the most of the people are Unmaried"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae804645",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## `BIVARIATE ANALYSIS`\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71125e43",
   "metadata": {},
   "source": [
    "---\n",
    "` Calculate average Purchase for each of the following columns: `\n",
    "--- \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec2b99b1",
   "metadata": {},
   "source": [
    "- **Gender**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0ae81c19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------------+------------------+\n",
      "|Gender|Gender_Count|Purchase by Gender|\n",
      "+------+------------+------------------+\n",
      "|     F|      135809|        1186232642|\n",
      "|     M|      414259|        3909580100|\n",
      "+------+------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# number of datapoints with each country\n",
    "train_data.groupBy(\"Gender\").agg(F.count(\"Gender\").alias(\"Gender_Count\"),\n",
    "                                    F.sum(\"Purchase\").alias(\"Purchase by Gender\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee62c9f1",
   "metadata": {},
   "source": [
    "- **Age**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5f012f4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---------+---------------+\n",
      "|  Age|Age_Count|Purchase by Age|\n",
      "+-----+---------+---------------+\n",
      "|18-25|    99660|      913848675|\n",
      "|26-35|   219587|     2031770578|\n",
      "| 0-17|    15102|      134913183|\n",
      "|46-50|    45701|      420843403|\n",
      "|51-55|    38501|      367099644|\n",
      "|36-45|   110013|     1026569884|\n",
      "|  55+|    21504|      200767375|\n",
      "+-----+---------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# number of datapoints with each country\n",
    "train_data.groupBy(\"Age\").agg(F.count(\"Age\").alias(\"Age_Count\"),\n",
    "                                    F.sum(\"Purchase\").alias(\"Purchase by Age\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d84afeb",
   "metadata": {},
   "source": [
    "- **City_Category**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a7336874",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-------------------+-------------------------+\n",
      "|City_Category|City_Category_Count|Purchase by City_Category|\n",
      "+-------------+-------------------+-------------------------+\n",
      "|            B|             231173|               2115533605|\n",
      "|            C|             171175|               1663807476|\n",
      "|            A|             147720|               1316471661|\n",
      "+-------------+-------------------+-------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_data.groupBy(\"City_Category\").agg(F.count(\"City_Category\").alias(\"City_Category_Count\"),\n",
    "                                       F.sum(\"Purchase\").alias(\"Purchase by City_Category\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d13ad896",
   "metadata": {},
   "source": [
    "- **Stay_In_Current_City_Years**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6f1c85c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------+--------------------------------+--------------------------------------+\n",
      "|Stay_In_Current_City_Years|Stay_In_Current_City_Years_Count|Purchase by Stay_In_Current_City_Years|\n",
      "+--------------------------+--------------------------------+--------------------------------------+\n",
      "|                         3|                           95285|                             884902659|\n",
      "|                         0|                           74398|                             682979229|\n",
      "|                        4+|                           84726|                             785884390|\n",
      "|                         1|                          193821|                            1792872533|\n",
      "|                         2|                          101838|                             949173931|\n",
      "+--------------------------+--------------------------------+--------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_data.groupBy(\"Stay_In_Current_City_Years\").agg(F.count(\"Stay_In_Current_City_Years\").alias(\"Stay_In_Current_City_Years_Count\"),\n",
    "                                                    F.sum('Purchase').alias(\"Purchase by Stay_In_Current_City_Years\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41d2e1be",
   "metadata": {},
   "source": [
    "- **Marital_Status**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3cbd7acd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+--------------+--------------------------+\n",
      "|Marital_Status|Marital_Status|Purchase by Marital_Status|\n",
      "+--------------+--------------+--------------------------+\n",
      "|             1|        225337|                2086885295|\n",
      "|             0|        324731|                3008927447|\n",
      "+--------------+--------------+--------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_data.groupBy(\"Marital_Status\").agg(F.count(\"Marital_Status\").alias(\"Marital_Status\"),\n",
    "                                        F.sum(\"Purchase\").alias(\"Purchase by Marital_Status\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4ae8b52",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## `Pre-Processing`\n",
    "\n",
    "---\n",
    "\n",
    "- We will do label-encoding the categorical variable.\n",
    "- We will do one-hot encoding the categorical variables.\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b89f2ce",
   "metadata": {},
   "source": [
    "---\n",
    "#### `Product_ID`\n",
    "##### Here, Product_ID is the ID of the different product, and hence it's a categorical variable.\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "546dd880",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StringIndexer\n",
    "plan_indexer = StringIndexer(inputCol = 'Product_ID', outputCol = 'product_ID')\n",
    "labeller = plan_indexer.fit(train_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1477a53e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = labeller.transform(train_data)\n",
    "test_data = labeller.transform(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27c68be2",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## `Label Encoding`\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6d19c5a",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "#### `Encoding Categorical Variables`\n",
    "\n",
    "Categorical values need to be encoded or converted to numeric form before inputting it to an ML model.\n",
    "\n",
    "Two ways of encoding categorical values:\n",
    "\n",
    "**Label Encoding**\n",
    "\n",
    "   > Assigning a unique integer to each of the categorical values\n",
    "   > Done using StringIndexer\n",
    "\n",
    "**One Hot Encoding**\n",
    "\n",
    "   > New columns created for each of the unique value in categorical column. Values assigned 0 or 1 based on the presence of the data.\n",
    "   > Requires features to be label encoded first.\n",
    "   > Done using OneHotEncoderEstimator\n",
    "    \n",
    "\n",
    "\n",
    "----\n",
    "\n",
    "We will do the following encoding of the categorical variables in this notebook.\n",
    "\n",
    " - Gender (One Hot Encode)\n",
    " - Age (One Hot Encode) \n",
    " - City_Category (One Hot Encode)\n",
    " - Stay_in_current_city_years      (One Hot Encode)    \n",
    " \n",
    "    \n",
    "    \n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7852db8",
   "metadata": {},
   "source": [
    "#### `Importing the Required Libraries`\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1e697dda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing some more libraries\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "from pyspark.ml.feature import OneHotEncoderEstimator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec31074b",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "#### `Creating the StringIndexer Objects`\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "522b001d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# label encode Gender\n",
    "SI_Gender = StringIndexer(inputCol= \"Gender\", outputCol= \"gender_le\" , handleInvalid=\"skip\")\n",
    "\n",
    "# label encode Age\n",
    "SI_Age = StringIndexer(inputCol= \"Age\", outputCol= \"age_le\", handleInvalid= \"skip\")\n",
    "\n",
    "# label encode City_Category\n",
    "SI_City_Category      = StringIndexer(inputCol= \"City_Category\",      outputCol= \"city_category_le\", handleInvalid= \"skip\")\n",
    "\n",
    "# label encode Stay_in_current_city_years\n",
    "SI_Stay_in_Current_City_Years = StringIndexer(inputCol= \"Stay_In_Current_City_Years\", outputCol= \"stay_in_current_city_years_le\", handleInvalid=\"skip\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bf4f893",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "#### `Fit the StringIndexer Objects`\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8949d6c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# label encode object Gender\n",
    "SI_Gender_Obj = SI_Gender.fit(train_data)\n",
    "\n",
    "# label encode object Age\n",
    "SI_Age_Obj = SI_Age.fit(train_data)\n",
    "\n",
    "# label encode object City_Category\n",
    "SI_City_Category_Obj = SI_City_Category.fit(train_data)\n",
    "\n",
    "# label encode object Stay_in_current_city_years\n",
    "SI_Stay_in_Current_City_Years_Obj = SI_Stay_in_Current_City_Years.fit(train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74429559",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "#### `Make the Transformation on the test data`\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "52f64982",
   "metadata": {},
   "outputs": [],
   "source": [
    "# label encode Gender\n",
    "train_data_encoded = SI_Gender_Obj.transform(train_data)\n",
    "\n",
    "# label encode Age\n",
    "train_data_encoded = SI_Age_Obj.transform(train_data_encoded)\n",
    "\n",
    "# label encode City_category\n",
    "train_data_encoded = SI_City_Category_Obj.transform(train_data_encoded)\n",
    "\n",
    "# label encode Stay_in_current_city_years\n",
    "train_data_encoded = SI_Stay_in_Current_City_Years_Obj.transform(train_data_encoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aaf049a",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "#### `Make the Transformation on the test data`\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "015ce082",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_encoded = SI_Gender_Obj.transform(test_data)\n",
    "\n",
    "test_data_encoded = SI_Age_Obj.transform(test_data_encoded)\n",
    "\n",
    "test_data_encoded = SI_City_Category_Obj.transform(test_data_encoded)\n",
    "\n",
    "test_data_encoded = SI_Stay_in_Current_City_Years_Obj.transform(test_data_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "edf2b757",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['User_ID',\n",
       " 'Product_ID',\n",
       " 'Gender',\n",
       " 'Age',\n",
       " 'Occupation',\n",
       " 'City_Category',\n",
       " 'Stay_In_Current_City_Years',\n",
       " 'Marital_Status',\n",
       " 'Product_Category_1',\n",
       " 'Product_Category_2',\n",
       " 'Product_Category_3',\n",
       " 'Purchase',\n",
       " 'product_ID',\n",
       " 'gender_le',\n",
       " 'age_le',\n",
       " 'city_category_le',\n",
       " 'stay_in_current_city_years_le']"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# let's check the columns in the data\n",
    "train_data_encoded.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9df5b22e",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### `ONE-HOT ENCODING`\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "812ba0d1",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Next, we will create the object of the `OneHotEncoderEstimator` and pass the input and output columns list.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "39439a54",
   "metadata": {},
   "outputs": [],
   "source": [
    "OHE_train = OneHotEncoderEstimator(inputCols=[\"gender_le\",\n",
    "                                              \"age_le\",\n",
    "                                              \"city_category_le\",\n",
    "                                              \"stay_in_current_city_years_le\"],\n",
    "                                  outputCols=[\"gender_ohe\",\n",
    "                                              \"age_ohe\",\n",
    "                                              \"city_category_ohe\",\n",
    "                                              \"stay_in_current_city_years_ohe\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bd89023",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Fit & Transform the data.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b08168d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OHE object\n",
    "OHE_Obj = OHE_train.fit(train_data_encoded)\n",
    "\n",
    "# Transform train data\n",
    "train_data_encoded = OHE_Obj.transform(train_data_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "833f5cde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------+-------------+-----------------+------------------------------+\n",
      "|User_ID|   gender_ohe|      age_ohe|city_category_ohe|stay_in_current_city_years_ohe|\n",
      "+-------+-------------+-------------+-----------------+------------------------------+\n",
      "|1000001|    (1,[],[])|    (6,[],[])|        (2,[],[])|                 (4,[1],[1.0])|\n",
      "|1000001|    (1,[],[])|    (6,[],[])|        (2,[],[])|                 (4,[1],[1.0])|\n",
      "|1000001|    (1,[],[])|    (6,[],[])|        (2,[],[])|                 (4,[1],[1.0])|\n",
      "|1000001|    (1,[],[])|    (6,[],[])|        (2,[],[])|                 (4,[1],[1.0])|\n",
      "|1000002|(1,[0],[1.0])|(6,[5],[1.0])|    (2,[1],[1.0])|                 (4,[3],[1.0])|\n",
      "|1000003|(1,[0],[1.0])|(6,[0],[1.0])|        (2,[],[])|                 (4,[2],[1.0])|\n",
      "|1000004|(1,[0],[1.0])|(6,[3],[1.0])|    (2,[0],[1.0])|                 (4,[1],[1.0])|\n",
      "|1000004|(1,[0],[1.0])|(6,[3],[1.0])|    (2,[0],[1.0])|                 (4,[1],[1.0])|\n",
      "|1000004|(1,[0],[1.0])|(6,[3],[1.0])|    (2,[0],[1.0])|                 (4,[1],[1.0])|\n",
      "|1000005|(1,[0],[1.0])|(6,[0],[1.0])|        (2,[],[])|                 (4,[0],[1.0])|\n",
      "|1000005|(1,[0],[1.0])|(6,[0],[1.0])|        (2,[],[])|                 (4,[0],[1.0])|\n",
      "|1000005|(1,[0],[1.0])|(6,[0],[1.0])|        (2,[],[])|                 (4,[0],[1.0])|\n",
      "|1000005|(1,[0],[1.0])|(6,[0],[1.0])|        (2,[],[])|                 (4,[0],[1.0])|\n",
      "|1000005|(1,[0],[1.0])|(6,[0],[1.0])|        (2,[],[])|                 (4,[0],[1.0])|\n",
      "|1000006|    (1,[],[])|(6,[4],[1.0])|        (2,[],[])|                 (4,[0],[1.0])|\n",
      "|1000006|    (1,[],[])|(6,[4],[1.0])|        (2,[],[])|                 (4,[0],[1.0])|\n",
      "|1000006|    (1,[],[])|(6,[4],[1.0])|        (2,[],[])|                 (4,[0],[1.0])|\n",
      "|1000006|    (1,[],[])|(6,[4],[1.0])|        (2,[],[])|                 (4,[0],[1.0])|\n",
      "|1000007|(1,[0],[1.0])|(6,[1],[1.0])|    (2,[0],[1.0])|                 (4,[0],[1.0])|\n",
      "|1000008|(1,[0],[1.0])|(6,[0],[1.0])|    (2,[1],[1.0])|                 (4,[3],[1.0])|\n",
      "+-------+-------------+-------------+-----------------+------------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# view the one hot encoded data\n",
    "train_data_encoded.select(\"User_ID\",\n",
    "                          \"gender_ohe\", \n",
    "                          \"age_ohe\",\n",
    "                          \"city_category_ohe\",\n",
    "                          \"stay_in_current_city_years_ohe\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d40c1766",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "#### `Make the transformation on test data`\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "0c8bbc89",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_encoded = OHE_Obj.transform(test_data_encoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf6558b3",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "\n",
    "## `MODEL BUILDING`\n",
    "\n",
    "Now, we have prepared the dataset and it is ready to be trained with Machine Learning models. This is a `Regression Problem`, so we will train the data on the following alogrithms.\n",
    "\n",
    " * **Linear Regression**\n",
    " * **Decision Tree Regression**\n",
    " * **RandomForest Regressor**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc8fd18a",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "#### `VECTOR ASSEMBLER`\n",
    "\n",
    "- Before passing the data into the ML model, we need to convert the required features into a Vector. We can do this using a `VectorAssembler`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "d29a1344",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['User_ID',\n",
       " 'Product_ID',\n",
       " 'Gender',\n",
       " 'Age',\n",
       " 'Occupation',\n",
       " 'City_Category',\n",
       " 'Stay_In_Current_City_Years',\n",
       " 'Marital_Status',\n",
       " 'Product_Category_1',\n",
       " 'Product_Category_2',\n",
       " 'Product_Category_3',\n",
       " 'Purchase',\n",
       " 'product_ID',\n",
       " 'gender_le',\n",
       " 'age_le',\n",
       " 'city_category_le',\n",
       " 'stay_in_current_city_years_le',\n",
       " 'gender_ohe',\n",
       " 'age_ohe',\n",
       " 'city_category_ohe',\n",
       " 'stay_in_current_city_years_ohe']"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data_encoded.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62d26fdd",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "#### `Define the VectorAssembler Object`\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "6d578b36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the library\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "# create feature vector\n",
    "feature_vector = VectorAssembler(inputCols= ['Marital_Status',\n",
    "                                             'gender_ohe',\n",
    "                                             'age_ohe',\n",
    "                                             'city_category_ohe',\n",
    "                                             'stay_in_current_city_years_ohe',\n",
    "                                            'Product_Category_1',\n",
    "                                            'Product_Category_2',\n",
    "                                            'Product_Category_3'],\n",
    "                                outputCol= 'feature_vector')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6204b66",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "`TRANSFORM THE FEATURE VECTOR`\n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "c5390f83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform the feature vector\n",
    "train_data_encoded = feature_vector.transform(train_data_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "eb86cb52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|      feature_vector|\n",
      "+--------------------+\n",
      "|(17,[11,14,15,16]...|\n",
      "|(17,[11,14,15,16]...|\n",
      "|(17,[11,14,15,16]...|\n",
      "|(17,[11,14,15,16]...|\n",
      "|(17,[1,7,9,13,14,...|\n",
      "|(17,[1,2,12,14,15...|\n",
      "|(17,[0,1,5,8,11,1...|\n",
      "|(17,[0,1,5,8,11,1...|\n",
      "|(17,[0,1,5,8,11,1...|\n",
      "|(17,[0,1,2,10,14,...|\n",
      "+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# view the feature vector\n",
    "train_data_encoded.select(\"feature_vector\").show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "b5f703f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- User_ID: integer (nullable = true)\n",
      " |-- Product_ID: string (nullable = true)\n",
      " |-- Gender: string (nullable = true)\n",
      " |-- Age: string (nullable = true)\n",
      " |-- Occupation: integer (nullable = true)\n",
      " |-- City_Category: string (nullable = true)\n",
      " |-- Stay_In_Current_City_Years: string (nullable = true)\n",
      " |-- Marital_Status: integer (nullable = true)\n",
      " |-- Product_Category_1: integer (nullable = true)\n",
      " |-- Product_Category_2: integer (nullable = false)\n",
      " |-- Product_Category_3: integer (nullable = false)\n",
      " |-- Purchase: integer (nullable = true)\n",
      " |-- product_ID: double (nullable = false)\n",
      " |-- gender_le: double (nullable = false)\n",
      " |-- age_le: double (nullable = false)\n",
      " |-- city_category_le: double (nullable = false)\n",
      " |-- stay_in_current_city_years_le: double (nullable = false)\n",
      " |-- gender_ohe: vector (nullable = true)\n",
      " |-- age_ohe: vector (nullable = true)\n",
      " |-- city_category_ohe: vector (nullable = true)\n",
      " |-- stay_in_current_city_years_ohe: vector (nullable = true)\n",
      " |-- feature_vector: vector (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# we can see that the data type of the `feature_vector` is vector\n",
    "train_data_encoded.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "740e3355",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "#### `Make the transformation on test data.`\n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "dba1846b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test data\n",
    "test_data_encoded = feature_vector.transform(test_data_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "a9cdde99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- User_ID: integer (nullable = true)\n",
      " |-- Product_ID: string (nullable = true)\n",
      " |-- Gender: string (nullable = true)\n",
      " |-- Age: string (nullable = true)\n",
      " |-- Occupation: integer (nullable = true)\n",
      " |-- City_Category: string (nullable = true)\n",
      " |-- Stay_In_Current_City_Years: string (nullable = true)\n",
      " |-- Marital_Status: integer (nullable = true)\n",
      " |-- Product_Category_1: integer (nullable = true)\n",
      " |-- Product_Category_2: integer (nullable = false)\n",
      " |-- Product_Category_3: integer (nullable = false)\n",
      " |-- Purchase: integer (nullable = true)\n",
      " |-- product_ID: double (nullable = false)\n",
      " |-- gender_le: double (nullable = false)\n",
      " |-- age_le: double (nullable = false)\n",
      " |-- city_category_le: double (nullable = false)\n",
      " |-- stay_in_current_city_years_le: double (nullable = false)\n",
      " |-- gender_ohe: vector (nullable = true)\n",
      " |-- age_ohe: vector (nullable = true)\n",
      " |-- city_category_ohe: vector (nullable = true)\n",
      " |-- stay_in_current_city_years_ohe: vector (nullable = true)\n",
      " |-- feature_vector: vector (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_data_encoded.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "68a8eef3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- User_ID: integer (nullable = true)\n",
      " |-- Product_ID: string (nullable = true)\n",
      " |-- Gender: string (nullable = true)\n",
      " |-- Age: string (nullable = true)\n",
      " |-- Occupation: integer (nullable = true)\n",
      " |-- City_Category: string (nullable = true)\n",
      " |-- Stay_In_Current_City_Years: string (nullable = true)\n",
      " |-- Marital_Status: integer (nullable = true)\n",
      " |-- Product_Category_1: integer (nullable = true)\n",
      " |-- Product_Category_2: integer (nullable = false)\n",
      " |-- Product_Category_3: integer (nullable = false)\n",
      " |-- product_ID: double (nullable = false)\n",
      " |-- gender_le: double (nullable = false)\n",
      " |-- age_le: double (nullable = false)\n",
      " |-- city_category_le: double (nullable = false)\n",
      " |-- stay_in_current_city_years_le: double (nullable = false)\n",
      " |-- gender_ohe: vector (nullable = true)\n",
      " |-- age_ohe: vector (nullable = true)\n",
      " |-- city_category_ohe: vector (nullable = true)\n",
      " |-- stay_in_current_city_years_ohe: vector (nullable = true)\n",
      " |-- feature_vector: vector (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_data_encoded.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4921cab",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "#### `LINEAR REGRESSION`\n",
    "\n",
    "First, we will train the model with the Linear Regression and see the `RMSE Score`.\n",
    "\n",
    "For that let's import some more libraries.\n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "4750a6a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the libraries\n",
    "from pyspark.ml import regression\n",
    "from pyspark.ml.evaluation import RegressionEvaluator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a8b5869",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "First we will create the Linear Regression and pass the `feature vector` and the label column which is `Purchase`\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "05e9fdd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_LR = regression.LinearRegression(featuresCol='feature_vector', labelCol=\"Purchase\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de90bb39",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "\n",
    "Fit the model\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "9ef03038",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_LR = model_LR.fit(train_data_encoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "324ca33b",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Next, we will use the `RegressionEvaluator` to calculate the `rmse score` and for that we have to pass the `labelCol: Purchase` and the `metricName: rmse`\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "b8cf007d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4681.23511028296"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluate training data\n",
    "evaluator = RegressionEvaluator(labelCol=\"Purchase\", metricName=\"rmse\") \n",
    "evaluator.evaluate(model_LR.transform(train_data_encoded)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "414dbd58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Use a regression evaluator to evaluate the predictions\n",
    "# evaluator = RegressionEvaluator(labelCol=\"Purchase\",metricName=\"rmse\")\n",
    "# evaluator.evaluate(model_RF.transform(train_data_encoded))\n",
    "# # rmse = evaluator.evaluate(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "4da20130",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate test data\n",
    "# evaluator.evaluate(model_LR.transform(test_data_encoded))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60c4a1bd",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "#### `DECISION TREE REGRESSION`\n",
    "\n",
    "\n",
    "Next, we will train the decision tree regression model and do the same steps as we did with the linear regression.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "0eb175ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_DTR = regression.DecisionTreeRegressor(featuresCol= \"feature_vector\",  labelCol=\"Purchase\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "71769ace",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit the model\n",
    "model_DTR = model_DTR.fit(train_data_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "cec195b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3255.036913818846"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# evaluate the model\n",
    "evaluator.evaluate(model_DTR.transform(train_data_encoded))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "5654c019",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on test data\n",
    "# evaluator.evaluate(model_DTR.transform(test_data_encoded))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3271695",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "#### `RANDOM FOREST REGRESSOR`\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "5482baed",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_RFR = regression.RandomForestRegressor(featuresCol= \"feature_vector\",  labelCol=\"Purchase\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "3cf453b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit the model\n",
    "model_RFR = model_RFR.fit(train_data_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "ad96540a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3901.0999790553083"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# evaluate the model\n",
    "evaluator.evaluate(model_RFR.transform(train_data_encoded))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "043f6ff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on test data\n",
    "# evaluator.evaluate(model_RFR.transform(test_data_encoded))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bd66931",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "So far, we saw the `Linear Regression`, `Decision Tree` and `RandomForestRegressor` algorithms. But we were running them using the default parameters only. So, we will tune the parameters of our model and see if it gets any better. \n",
    "\n",
    "First we, will do the cross validation\n",
    "\n",
    "---\n",
    "\n",
    "## `CROSS VALIDATION`\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Let's see how to do the cross validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "a9a69cc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the CrossValidator and ParamGridBuilder\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "42428477",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the ParamGridBuilder\n",
    "params = ParamGridBuilder().build()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e40068aa",
   "metadata": {},
   "source": [
    "#### `Linear Regression cross-validation`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "b3cb2450",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the object of the Logistic Regression Model\n",
    "model_LR_CV = regression.LinearRegression(featuresCol= \"feature_vector\",  labelCol=\"Purchase\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "b71c8be3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create object of the cross validation model with numFolds = 3\n",
    "cv = CrossValidator(estimator=model_LR_CV,\n",
    "                    estimatorParamMaps=params,\n",
    "                    evaluator=evaluator, # which is auc-roc score\n",
    "                    numFolds=3,\n",
    "                    seed=27)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "77789eed",
   "metadata": {},
   "outputs": [],
   "source": [
    "## fit the model\n",
    "cv_model = cv.fit(train_data_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "592cd41d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4681.23511028296"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# evaluate the model\n",
    "evaluator = RegressionEvaluator(labelCol=\"Purchase\",metricName=\"rmse\") \n",
    "evaluator.evaluate(cv_model.transform(train_data_encoded)) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bee285c0",
   "metadata": {},
   "source": [
    "#### `DecisionTreeRegressor cross-validation`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "a9f3da60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the object of the DecisionTreeRegressor Model\n",
    "model_DTR_CV = regression.DecisionTreeRegressor(featuresCol= \"feature_vector\",  labelCol=\"Purchase\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "5ca3f217",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create object of the cross validation model with numFolds = 3\n",
    "cv = CrossValidator(estimator=model_DTR_CV,\n",
    "                    estimatorParamMaps=params,\n",
    "                    evaluator=evaluator, # which is auc-roc score\n",
    "                    numFolds=3,\n",
    "                    seed=27)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "eed613c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "## fit the model\n",
    "cv_model = cv.fit(train_data_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "3256a12e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3255.036913818846"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# evaluate the model\n",
    "evaluator = RegressionEvaluator(labelCol=\"Purchase\",metricName=\"rmse\") \n",
    "evaluator.evaluate(cv_model.transform(train_data_encoded)) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abe8155e",
   "metadata": {},
   "source": [
    "#### `RandomForestRegressor cross validation`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "c2801b95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the object of the DecisionTreeRegressor Model\n",
    "model_RFR_CV = regression.RandomForestRegressor(featuresCol= \"feature_vector\",  labelCol=\"Purchase\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "cd3486cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create object of the cross validation model with numFolds = 3\n",
    "cv = CrossValidator(estimator=model_RFR_CV,\n",
    "                    estimatorParamMaps=params,\n",
    "                    evaluator=evaluator, # which is auc-roc score\n",
    "                    numFolds=3,\n",
    "                    seed=27)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "5f59c6c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "## fit the model\n",
    "cv_model = cv.fit(train_data_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "18d071b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3901.0999790553083"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# evaluate the model\n",
    "evaluator = RegressionEvaluator(labelCol=\"Purchase\",metricName=\"rmse\") \n",
    "evaluator.evaluate(cv_model.transform(train_data_encoded)) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef9792db",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "\n",
    "## `GRID SEARCH`\n",
    "\n",
    "Next, we will try the Grid Search where it will apply different combinations of parameter to give you the best model.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7c1dab6",
   "metadata": {},
   "source": [
    "#### `Hyper-parameter Tunning of the model using Grid Search in Linear Regression`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "f56fd652",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create parameter builder\n",
    "\n",
    "updated_params = ParamGridBuilder() \\\n",
    "                .addGrid(model_LR_CV.regParam, [0.01, 0.005, 0.0001]) \\\n",
    "                .addGrid(model_LR_CV.elasticNetParam, [0.1, 0.001]) \\\n",
    "                .build()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "8cdb2d7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create object of the Cross Calidator with 3 folds\n",
    "cv = CrossValidator(estimator=model_LR_CV,\n",
    "                    estimatorParamMaps=updated_params,\n",
    "                    evaluator=evaluator,\n",
    "                    numFolds=3,\n",
    "                    seed=27)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "14ed8a8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit the model\n",
    "grid_model = cv.fit(train_data_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "b375721d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4681.235110344878"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# evaluate the model\n",
    "evaluator = RegressionEvaluator(labelCol=\"Purchase\",metricName=\"rmse\") \n",
    "evaluator.evaluate(grid_model.transform(train_data_encoded))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac907abb",
   "metadata": {},
   "source": [
    "#### `Hyper-parameter Tunning of the model using Grid Search in DecisionTree Regression`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "b87a43a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create parameter builder\n",
    "\n",
    "updated_params = ParamGridBuilder() \\\n",
    "                .addGrid(model_DTR_CV.maxDepth, [3, 5, 10, 15]) \\\n",
    "                .addGrid(model_DTR_CV.maxBins, [10, 20, 30, 50, 100]) \\\n",
    "                .build()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "0a47d9d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create object of the Cross Validator with 3 folds\n",
    "cv = CrossValidator(estimator=model_DTR_CV,\n",
    "                    estimatorParamMaps=updated_params,\n",
    "                    evaluator=evaluator,\n",
    "                    numFolds=3,\n",
    "                    seed=27)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "40b7136e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit the model\n",
    "grid_model = cv.fit(train_data_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "6a23cdf7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2938.191126866426"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# evaluate the model\n",
    "evaluator = RegressionEvaluator(labelCol=\"Purchase\",metricName=\"rmse\") \n",
    "evaluator.evaluate(grid_model.transform(train_data_encoded))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d77456cb",
   "metadata": {},
   "source": [
    "#### `Hyper-parameter Tunning of the model using Grid Search in RandomForest Regression`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "761686b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "updated_params = ParamGridBuilder() \\\n",
    "                .addGrid(model_RFR_CV.maxDepth, [5, 10, 15]) \\\n",
    "                .addGrid(model_RFR_CV.numTrees, [10, 20, 30, 40]) \\\n",
    "                .build()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "059779de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create object of the Cross Validator with 3 folds\n",
    "cv = CrossValidator(estimator=model_RFR_CV,\n",
    "                    estimatorParamMaps=updated_params,\n",
    "                    evaluator=evaluator,\n",
    "                    numFolds=3,\n",
    "                    seed=27)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "8fa63560",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit the model\n",
    "grid_model = cv.fit(train_data_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "b4ab634b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2974.408913947976"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# evaluate the model\n",
    "evaluator = RegressionEvaluator(labelCol=\"Purchase\",metricName=\"rmse\") \n",
    "evaluator.evaluate(grid_model.transform(train_data_encoded))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43df512e",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Get the best model parameters.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "a780933c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract the best model parameters dictionary\n",
    "param_dict = grid_model.bestModel.extractParamMap()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "0ad9b644",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{Param(parent='RandomForestRegressor_5536294304f1', name='cacheNodeIds', doc='If false, the algorithm will pass trees to executors to match instances with nodes. If true, the algorithm will cache node IDs for each instance. Caching can speed up training of deeper trees.'): False,\n",
       " Param(parent='RandomForestRegressor_5536294304f1', name='checkpointInterval', doc='set checkpoint interval (>= 1) or disable checkpoint (-1). E.g. 10 means that the cache will get checkpointed every 10 iterations. Note: this setting will be ignored if the checkpoint directory is not set in the SparkContext'): 10,\n",
       " Param(parent='RandomForestRegressor_5536294304f1', name='featureSubsetStrategy', doc='The number of features to consider for splits at each tree node. Supported options: auto, all, onethird, sqrt, log2, (0.0-1.0], [1-n].'): 'auto',\n",
       " Param(parent='RandomForestRegressor_5536294304f1', name='featuresCol', doc='features column name'): 'feature_vector',\n",
       " Param(parent='RandomForestRegressor_5536294304f1', name='impurity', doc='Criterion used for information gain calculation (case-insensitive). Supported options: variance'): 'variance',\n",
       " Param(parent='RandomForestRegressor_5536294304f1', name='labelCol', doc='label column name'): 'Purchase',\n",
       " Param(parent='RandomForestRegressor_5536294304f1', name='maxBins', doc='Max number of bins for discretizing continuous features.  Must be at least 2 and at least number of categories for any categorical feature.'): 32,\n",
       " Param(parent='RandomForestRegressor_5536294304f1', name='maxDepth', doc='Maximum depth of the tree. (Nonnegative) E.g., depth 0 means 1 leaf node; depth 1 means 1 internal node + 2 leaf nodes.'): 15,\n",
       " Param(parent='RandomForestRegressor_5536294304f1', name='maxMemoryInMB', doc='Maximum memory in MB allocated to histogram aggregation.'): 256,\n",
       " Param(parent='RandomForestRegressor_5536294304f1', name='minInfoGain', doc='Minimum information gain for a split to be considered at a tree node.'): 0.0,\n",
       " Param(parent='RandomForestRegressor_5536294304f1', name='minInstancesPerNode', doc='Minimum number of instances each child must have after split.  If a split causes the left or right child to have fewer than minInstancesPerNode, the split will be discarded as invalid. Must be at least 1.'): 1,\n",
       " Param(parent='RandomForestRegressor_5536294304f1', name='numTrees', doc='Number of trees to train (at least 1)'): 40,\n",
       " Param(parent='RandomForestRegressor_5536294304f1', name='predictionCol', doc='prediction column name'): 'prediction',\n",
       " Param(parent='RandomForestRegressor_5536294304f1', name='seed', doc='random seed'): 4675562443428127417,\n",
       " Param(parent='RandomForestRegressor_5536294304f1', name='subsamplingRate', doc='Fraction of the training data used for learning each decision tree, in range (0, 1].'): 1.0}"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "504e16a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# created a filtered dictionary\n",
    "final_dict = {}\n",
    "for k, v in param_dict.items():\n",
    "    final_dict[k.name] = v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "ee366adc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get the best elastic net parameter\n",
    "final_dict[\"maxDepth\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "fc6b1608",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get the best regularization parameter\n",
    "final_dict[\"numTrees\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b6ea150",
   "metadata": {},
   "source": [
    "\n",
    "----\n",
    "----\n",
    "\n",
    "\n",
    "### `Machine Learning Pipelines in Spark`\n",
    "\n",
    "----\n",
    "\n",
    "\n",
    " *   `Transformer` \n",
    " > - It transforms the input data (X) in some ways.\n",
    " > - Implements a method `transform()`, which converts one DataFrame into another, generally by appending one or more columns.\n",
    " > - It includes `feature transformers` and `learned models`.\n",
    " > > - `Feature transformer` should take a DataFrame, read a column (e.g., text), map it into a new column (e.g., feature vectors), and output a new DataFrame with the mapped column appended.\n",
    " > > -  `Learning model` should take a DataFrame, read the column containing feature vectors, predict the label for each feature vector, and output a new DataFrame with predicted labels appended as a column.\n",
    " *   `Estimator` \n",
    " > - It predicts a new value (or values) (y) by using the input data (X).\n",
    " > - It implements a method `fit()`, which accepts a DataFrame and produces a `Model`, which is a `Transformer`.\n",
    " > > - For example, a learning algorithm such as `LogisticRegression` is an `Estimator`, and calling `fit()` trains a `LogisticRegressionModel`, which is a `Model` and hence a `Transformer`.\n",
    " * `Pipeline`\n",
    " > - It represents a sequence of steps to apply in an ML workflow. Example:\n",
    " > > - Stage 1 : Split text into words.\n",
    " > > - Stage 2 : Convert words into numeric data.\n",
    " > > - Stage 3 : Apply machine learning model on the numeric data.\n",
    " > - These steps are represented as `Transformers` or as `Estimators`.\n",
    " > - A `Pipeline` is comprised of `Stages`.\n",
    " > > - These stages are run in order.\n",
    " > > - The input DataFrame is transformed as it passes through each stage.\n",
    " > > - Each stage is either a `Transformer` or an `Estimator`.\n",
    " \n",
    " ---\n",
    " \n",
    "**`Import the Required Libraries`**\n",
    " \n",
    " ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "d80ea6fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import the Required Libraries \n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import OneHotEncoderEstimator, StringIndexer, VectorAssembler\n",
    "from pyspark.ml import Transformer\n",
    "from pyspark.ml.param.shared import HasInputCol, HasOutputCol, Param\n",
    "from pyspark.ml import regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "dbfc10bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reading the train data\n",
    "train_data = spark.read.csv(\"ml_assignment/train.csv\",inferSchema=True, header=True)\n",
    "\n",
    "# reading the test data\n",
    "test_data  = spark.read.csv(\"ml_assignment/test.csv\", inferSchema=True, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "613d6681",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data=train_data.dropDuplicates()\n",
    "test_data=test_data.dropDuplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "fe0daa84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pyspark.ml.feature import StringIndexer\n",
    "# plan_indexer = StringIndexer(inputCol = 'Product_ID', outputCol = 'product_ID')\n",
    "# labeller = plan_indexer.fit(train_data)\n",
    "\n",
    "# train_data = labeller.transform(train_data)\n",
    "# test_data = labeller.transform(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "b70a973e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_data.select('Product_ID').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ea3322f",
   "metadata": {},
   "source": [
    "---\n",
    "### `Define the stages of the Pipeline`\n",
    "\n",
    "  * **STAGE 1**: [Transformer] Fill null values with in each column\n",
    "  * **STAGE 2**: [Estimator] Label Encode: Age\n",
    "  * **STAGE 4**: [Estimator] Label Encode: Gender\n",
    "  * **STAGE 5**: [Estimator] Label Encode: City_Category\n",
    "  * **STAGE 6**: [Estimator] Label Encode: Stay_In_Current_City_Years\n",
    "  * **STAGE 7**: [Estimator] OHE: Age,Gender,City_Category,Stay_In_Current_City_Years\n",
    "  * **STAGE 8**: [Transformer] Create Feature: Total Click per Publisher ID, Total Click per Campaign ID\n",
    "  * **STAGE 9**: Transformer] Create Vector [Marital_Status, gender_ohe, age_ohe, city_category_ohe, product_category_1, product_category_2, product_category_3,]\n",
    "  * **STAGE 10**: [Estimator] Predict sales Using the Linear Regression\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c31d5ac1",
   "metadata": {},
   "source": [
    "----\n",
    "#### `Custom Transformer to fill Null Values`\n",
    "\n",
    "* Fill null value with mode from respective column.\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "763b6d82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# custom transformer to fill null values\n",
    "\n",
    "class nullValuesTransformer(Transformer):\n",
    "    \n",
    "    def __init__(self, dataframe = None):\n",
    "        self.dataframe = dataframe\n",
    "    \n",
    "    def _transform(self, dataframe):\n",
    "        dataframe = dataframe.fillna({\n",
    "            \"Product_Category_2\" : 8,\n",
    "            \"Product_Category_3\": 16,\n",
    "        })\n",
    "        \n",
    "        return dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5bb6bce",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "\n",
    "#### **`Define the Stages`**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "83a9de52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stage 1 - replace null values\n",
    "stage_1 = nullValuesTransformer()\n",
    "\n",
    "# Stage 2 - label encode Age column\n",
    "stage_2 = StringIndexer(inputCol= \"Age\", outputCol= \"age_le\") \n",
    "\n",
    "# Stage 3 - label encode Gender column\n",
    "stage_3 = StringIndexer(inputCol= \"Gender\", outputCol= \"gender_le\")\n",
    "\n",
    "# Stage 4 - label encode City_Category column\n",
    "stage_4 = StringIndexer(inputCol= \"City_Category\", outputCol= \"city_category_le\")\n",
    "\n",
    "# Stage 5 - label encode Stay_In_Current_City_Years column\n",
    "stage_5 = StringIndexer(inputCol= \"Stay_In_Current_City_Years\", outputCol= \"stay_in_current_city_years_le\")\n",
    "\n",
    " # Stage 6 - One Hot Encode columns\n",
    "stage_6 = OneHotEncoderEstimator(inputCols= [\"age_le\",  \"gender_le\", \"city_category_le\", \"stay_in_current_city_years_le\"], \n",
    "                        outputCols= [\"age_ohe\",  \"gender_ohe\", \"city_category_ohe\", \"stay_in_current_city_years_ohe\"])\n",
    "\n",
    "# Stage 7 - Create vector from the columns\n",
    "stage_7 = VectorAssembler(inputCols= [\"Marital_Status\",\n",
    "                                      \"gender_ohe\",\n",
    "                                      \"age_ohe\",\n",
    "                                      \"city_category_ohe\",\n",
    "                                      \"stay_in_current_city_years_ohe\",\n",
    "                                      \"Product_Category_1\",\n",
    "                                      'Product_Category_2',\n",
    "                                      'Product_Category_3'],\n",
    "\n",
    "                         outputCol=  \"feature_vector\")\n",
    "\n",
    "# Stage 8 - Train ML model\n",
    "stage_8 = regression.LinearRegression(featuresCol= \"feature_vector\", labelCol= \"Purchase\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "42d6d91f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import RandomForestRegressor\n",
    "# from xgboost import XGBRegressor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7673b1e",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "#### **`Define the Pipeline`**\n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "aacf67aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define pipeline\n",
    "pipeline = Pipeline(stages= [stage_1,\n",
    "                             stage_2,\n",
    "                             stage_3,\n",
    "                             stage_4,\n",
    "                             stage_5,\n",
    "                             stage_6,\n",
    "                             stage_7,\n",
    "                             stage_8])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0047b73",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### `FIT AND TRANSFORM THE PIPELINE`\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "4dae19ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit the pipeline with the training data\n",
    "pipeline_model = pipeline.fit(train_data)\n",
    "\n",
    "# transform data\n",
    "final_data = pipeline_model.transform(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "b4185243",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------+------------------+\n",
      "|User_ID|Purchase|        prediction|\n",
      "+-------+--------+------------------+\n",
      "|1005733|   20173|  8894.16869832875|\n",
      "|1005763|    5948|  7332.05200616034|\n",
      "|1005776|   16145| 8611.449903199169|\n",
      "|1005892|    6447|11461.125295105072|\n",
      "|1005896|    6903|  9142.32888823947|\n",
      "|1005916|    4524| 7243.432309924363|\n",
      "|1005954|   13371|11707.597847815692|\n",
      "|1006001|   16626|4079.0333730525726|\n",
      "|1006020|   18986| 7462.647981394239|\n",
      "|1006034|   14290|7092.2106571772665|\n",
      "+-------+--------+------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## The prediction columns give the final prediction of the each record.\n",
    "final_data.select(\"User_ID\", \"Purchase\", \"prediction\").show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d50de98",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "#### `TRANSFORM THE TEST DATA`\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "daf375dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_test_data = pipeline_model.transform(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "d99f9fae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+\n",
      "|User_ID|        prediction|\n",
      "+-------+------------------+\n",
      "|1004333|10171.970185642247|\n",
      "|1004344| 9928.341702617017|\n",
      "|1005108| 8705.495393537345|\n",
      "|1005126| 9306.711209926105|\n",
      "|1005184|12357.249251508574|\n",
      "|1005618|11065.235757683005|\n",
      "|1005831| 9712.568353911254|\n",
      "|1006016| 9309.478953665905|\n",
      "|1000194| 8704.077685216555|\n",
      "|1000401|  7866.32436155466|\n",
      "+-------+------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "final_test_data.select(\"User_ID\", \"prediction\").show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ba3cf3a",
   "metadata": {},
   "source": [
    "### RandomForestRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "3a937d62",
   "metadata": {},
   "outputs": [],
   "source": [
    "stage_8 = regression.RandomForestRegressor(featuresCol= \"feature_vector\",  labelCol=\"Purchase\")\n",
    "# RandomForestRegressor\n",
    "# stage_8 = regression.LinearRegression(featuresCol= \"feature_vector\", labelCol= \"Purchase\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "f6139e0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline(stages= [stage_1,\n",
    "                             stage_2,\n",
    "                             stage_3,\n",
    "                             stage_4,\n",
    "                             stage_5,\n",
    "                             stage_6,\n",
    "                             stage_7,\n",
    "                             stage_8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "3407e767",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit the pipeline with the training data\n",
    "pipeline_model = pipeline.fit(train_data)\n",
    "\n",
    "# transform data\n",
    "final_data = pipeline_model.transform(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "308fce86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------+------------------+\n",
      "|User_ID|Purchase|        prediction|\n",
      "+-------+--------+------------------+\n",
      "|1003934|    3233| 7676.642549397426|\n",
      "|1003951|   11423|13851.081414726155|\n",
      "|1004085|    6179| 7392.062624473605|\n",
      "|1004085|    9987| 7392.062624473605|\n",
      "|1004120|    3643| 6830.163459904829|\n",
      "|1004144|   10079|10907.889218440354|\n",
      "|1004161|    1979| 6774.953314816991|\n",
      "|1004183|    8701| 6774.953314816991|\n",
      "|1004220|    7080|  6776.83638229648|\n",
      "|1004276|   10846| 9441.981941666798|\n",
      "+-------+--------+------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## The prediction columns give the final prediction of the each record.\n",
    "final_data.select(\"User_ID\", \"Purchase\", \"prediction\").show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "3ac33571",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_test_data = pipeline_model.transform(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "cef91128",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+\n",
      "|User_ID|        prediction|\n",
      "+-------+------------------+\n",
      "|1001536|14474.086618869982|\n",
      "|1001737| 7871.654885363244|\n",
      "|1001831|11591.816684048372|\n",
      "|1003191| 7865.750632144918|\n",
      "|1003216| 7904.339363412752|\n",
      "|1003227| 6869.981750300331|\n",
      "|1003230|13299.871364640072|\n",
      "|1003280| 7676.642549397426|\n",
      "|1003589| 7095.651645822142|\n",
      "|1003929|13007.018372148981|\n",
      "+-------+------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "final_test_data.select(\"User_ID\", \"prediction\").show(10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
